% Thomas Hales
% The Formal Proof of the Kepler Conjecture in Retrospective
% February 17, 2020. started.


%\documentclass[runningheads]{llncs}
%\documentclass{llncs}
\documentclass{amsart}

\usepackage{graphicx}
\usepackage{pdfpages}
%\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}
\usepackage{ellipsis}

% For code snippets
\usepackage{isabelle}
\usepackage{isabellesym}
\usepackage{isabelletags}
\usepackage{comment}
\usepackage{pdfsetup}
\usepackage{railsetup}
\usepackage{wasysym}

% Bibliography
% Change style to the requested style of the template
%\usepackage[backend=bibtex, style=numeric]{biblatex}
%\addbibresource{refs.bib}

% 
%tikz graphics
%\usepackage{xcolor} % to remove color.
\usepackage{tikz} % 
\usetikzlibrary{chains,shapes,arrows,%
 trees,matrix,positioning,decorations}
%\usepackage[framemethod=TikZ]{mdframed}

\def\tikzfig#1#2#3{%
\begin{figure}[htb]%
  \centering
\begin{tikzpicture}#3
\end{tikzpicture}
  \caption{#2}
  \label{fig:#1}%
\end{figure}%
}
\def\smalldot#1{\draw[fill=black] (#1) %
 node [inner sep=1.3pt,shape=circle,fill=black] {}}

%\newtheorem{theorem}{Theorem}[subsection]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{definition}[theorem]{Definition}

\newcommand{\ring}[1]{\mathbb{#1}}
\newcommand{\op}[1]{\hbox{#1}}

\title{The Formal Proof of the Kepler Conjecture: a critical retrospective}
\author{Thomas Hales}
\date{}
%\institute{University of Pittsburgh}
\date{February 28, 2020}

\begin{document}

\maketitle

\begin{abstract} 
  The Kepler conjecture asserts that no packing of congruent balls in
  Euclidean three-space has density greater than that of the
  face-centered cubic packing.  In 1998, Samuel Ferguson and Thomas
  Hales announced a computer-assisted proof of this conjecture.  Long
  delays in the refereeing process sparked a project to give a formal
  proof of the Kepler conjecture.  This article gives an overview of
  the formal proof of the Kepler Conjecture.
\end{abstract}

\parskip=0.8\baselineskip
\baselineskip=1.05\baselineskip

\newenvironment{blockquote}{%
  \par%
  \medskip%
  \baselineskip=0.7\baselineskip%
  \leftskip=2em\rightskip=2em%
  \noindent\ignorespaces}{%
  \par\medskip}

\section{Introduction}

In the late 16th century, Walter Raleigh asked his assistant Thomas
Harriot to compute the number of cannonballs piled in a pyramid (in
defense against the Spanish Armada).  Thomas Harriot obtained a
general formula (as the binomial coefficient $\choose{a}{b}$) for the
number of congruent balls piled in a pyramid of any size $k$ in any
dimension $d$.  In two dimensions, Harriot's formula reduces to the
formula $k(k+1)/2$ for triangular numbers.  In three dimensions,
Harriot's formula reduces to a formula known from antiquity in a
Sanskrit source.  The cannonball pyramid is known in chemistry as the
face-centered cubic packing.

Harriot was an early proponent of the atomic theory, and he viewed
atoms as small balls -- which might almost be viewed as miniature
cannonballs.  Harriot and Kepler shared an interest in optics, and
they corresponded during the first decade of the 17th century.
Harriot pushed the atomic theory in their correspondence, but Kepler
remained skeptical.

However, one evening crossing the Charles bridge in Prague, as it
started to snow, Kepler started to contemplate why snowflakes have six
sides.  His contemplations led to a booklet ``The Six-Cornered
Snowflake,'' which can be described as an early essay in
crystallography, explaining observable symmetries in nature by
arrangements of invisibly small particles.  In this booklet, published
in 1611, Kepler describes the face-centered cubic packing and asserts
that it will be the ``tightest possible, so that in no other
arrangement can more pellets be stuffed into the same space.'' The
notion of density was suggested to him by the tightly packed seeds of
a pomegranite, and the face-centered cubic packing was suggested to
him by the structure of honeycomb cells.

Kepler's assertion has come to be known as the Kepler conjecture.
Hilbert made the Kepler conjecture part of the 18th problem, one among
the influential list of problems he proposed at the international
congress in Paris in 1900.

The two-dimensional analogue of the Kepler conjecture asserts that the
densest packing of congruent circular disks in the plane is achieved
by the hexagonal lattice packing.  A. Thue is often credited with the
proof, although the first careful proofs do not appear until much
later.

The first detailed strategy to prove the Kepler conjecture was
formulated by L. Fejes T\'oth in 1953.  He was also the first to
suggest a computer-assisted proof.

Over the years, there were some notorious false claims of a proof.
Buckminster Fuller (the creator of the geodesic dome) claimed a proof,
but his claim lacked any substance.  Around 1990, a Berkeley professor
W.-Y. Hsiang claimed a proof.  In rebuttal to his claimed proof, in a
heated debate, I published a paper containing explicit counterexamples
to his work.  For a few years in the early 1990s, the research area
became toxic, until the debate was eventually settled in my favor.

Samuel Ferguson and I annouced a solution to the Kepler conjecture in
August 1998.  Our solution was contained in a series of preprints
posted to the ArXiv.  Because of the conjecture's notorious history, I
had hoped that publication of our papers would be swift. This was not
to be.  A panel of 12 referees was assigned the review.  Because of
lingering doubts among the referees, the full publication of the proof
did not occur until 2006, nearly eight years after submission.

During those years in delay, growing out of my frustration, in an
effort to bypass the referees, I launched a project to give a formal
proof of the Kepler conjecture.  Although the project was born out of
frustration, in truth, it was also
shaped by a broader vision of the central importance of
computer-assisted mathematics in years to come.  In a large group
collaboration, the formal proof of the Kepler conjecture was completed
in 2014.

\section{Statement}

The Kepler conjecture asserts that no packing of congruent balls in
Euclidean three-space has density greater than that of the
face-centered cubic packing.  This section gives an outline of the
proof.

The statement is invariant under rescalings of the radius of the balls,
and it is convenient to normalize the radius of each ball at $r=1$.
The congruent balls do not enter directly. Rather we identify the packing
with the set $V$ of centers of the balls.  The radius normalization implies
that every two distinct points in $V$ are separated by distance at least
$2 = 2r$.

As was known to Gauss, among lattices, the face-centered cubic packing
is the densest possible.

The face-centered cubic packing is not the only packing of congruent
balls that achieves the density $\pi/\sqrt{18} \approx 0.74$ of the
face-centered cubic packing.  The best known alternative is the
hexagonal close packing, which is also built by stacking hexagonal
planar layers of balls.  However, in the hexagonal close packing the
layers are shifted so that the balls of one layer fits into different
sets of pockets of the previous layer.  Infinitely many different
close packings are obtained in this manner by shifting hexagonal
layers to fit into different patterns of pockets in the previous
layer.  The proof does not include a uniqueness statement.

The Kepler conjecture is defined formally in HOL Light by the statement
\begin{verbatim}
`the_kepler_conjecture <=>
     (!V. packing V
            ==> (?c. !r. &1 <= r
                         ==> &(CARD(V INTER ball(vec 0,r))) <=
                             pi * r pow 3 / sqrt(&18) + c * r pow 2))`
\end{verbatim}
In HOL Light, ascii replacements of logical symbols are used:
the character $!$ stands for a universal quantifier $\forall$,
the character $?$ stands for an existential quantifier $\exists$.
Also, \& stands for the explicit embedding of the set $\ring{N}$ of natural
numbers into the field $\ring{R}$ of real numbers.  In a slightly
cleaned up version of the formal statement given above, we might write
\begin{align*}
&\op{the\_kepler\_conjecture} \iff\\
&\quad (\forall V.\ \op{packing}\ V\\
&\quad\quad \Longrightarrow (\exists c.\ \forall r.\ 1 \le r \Longrightarrow
\op{card}(V \cap B(0,r)) \le
\frac{\pi r^3}{\sqrt{18}} + c r^2)).
\end{align*}
To simplify the main statement as much as possible, the Kepler
conjecture is formulated as a statement about a finite
packing of balls within a large spherical container of radius $r$.  By
considering finite packings in a bounded container, boundary effects
of the container come into play, and an error term $c/r$ enters into
the main estimate (after dividing both sides of the inequality by $r^3$).  
The error $c/r$ tends to zero as the radius $r$ of
the container tends to infinity.
The condition $1\le r$ is an explicit way of encoding the
condition that $r$ should be sufficiently large.  

The constant $c$ in
the error term depends on the packing $V$ and is not explicit.  In
fact, the constant $c$ can be chosen to be independent of $V$ and one
nonoptimal value has been calculated to be $c=24373$ in an informal
calculation \cite{XX}.
% Nadja Sharf.
% https://arxiv.org/pdf/1712.03568.pdf

For simplicity, we took pains to remove all mention of
measure, which would naturally appear when speaking of densities.  To
push simplicity further, even $\pi$ might have been eliminated from
the statement, if we had used a large cubical container rather than a
large spherical container.



\section{proof sketch}

\subsection{Partitioning space}

In a major reduction, the proof is reduced to a local statement
about a single ball and its immediately neighboring balls.
This reduction relies on a geometric partition of space into cells,
which is adapted to the packing $V$ (viewed as always as the set of centers
of balls).

There is an enormous amount of freedom in choosing the partition
of space.  Much of Sam's and my effort in proving the Kepler conjecture
went into the design of a good partition of space.  

The most obvious choice of partition is the Voronoi cells
decomposition, which assigns (to each point in a packing $V$) the
polyhedral region of all points closer to that point than to any other
point in the packing $V$. This leads to a proof of the two-dimensional
analogue of the Kepler conjecture.  However, this approach does not
give the desired sharp bound in three dimensions, because there are
Voronoi cells (such as a regular dodecahedron) that have smaller
volume than the volume of the Voronoi cell (a rhombic dodecahedron) of
the face-centered cubic packing.

In 1953, L. Fejes T\'oth modified the Voronoi cells so that to each
point he associates a weighted linear combination of two layers of
truncated Voronoi cells.  In this form, he conjectured that the
corresponding bound on density is sharp.  Fejes T\'oth's conjecture is
plausible, but still unproved.  From a both combinatorial and
computational points of view, it is unwieldy.  A slight modification
of Fejes T\'oth's conjecture (that truncates Voronoi cells in a
polyhedral way) can be formulated as a first-order statement for
real-closed fields.  Thus, Tarski's algorithm could in principle be
applied to decide the modified Fejes T\'oth conjecture, which implies
the Kepler conjecture.  However, this is thoroughly impractical.

Starting around 1989, I started toying with other geometric partitions
of space.  The Delaunay decomposition (which is dual to the Voronoi
cell decomposition) was one of my first attempts.  In 1994, I started
to experiment with hybrid partitions that combined the best features
of both the Voronoi and Delaunay partitions.  We called the cells of
the hybrid partitions \emph{decomposition stars}.  A good partition
enabled computer calculations that ran in acceptable time.

The original proof from 1998 was based on decomposition stars.  As I
revised the text in preparation for formalization, 
the text started to grow to alarming lengths.  At one point, as I 
recall, the revised proof text was nearly 600 pages, and I feared that it would
reach 1000.  Much of this length stemmed from the complexity of
the decomposition star decomposition.  

The situation abrubtly changed in 2007, when C. Marchal introduced a
remarkable new geometric partition of space, tailored to the Kepler
conjecture.  His partition functioned in much the same way as
decomposition stars, but they had the effect of moving the
complexities from the text part of the proof (at the expense of more
cases and longer computer calculations).  At first, Marchal claimed to
have a new proof of the Kepler conjecture, but inspection of his work
showed that the computer calculations were highly incomplete, and he
backed away from the claim.\footnote{Marchal's work contains ingenious
  ideas, but on superficial inspection, his paper looked suspicious:
  an $18$-page paper with hand-drawn figures, poor typesetting, and
  trivial appearance, which nonetheless claimed a new proof of the
  Kepler conjecture. It is a great reminder not to judge too quickly.}
Starting in 2007, abandoning the decomposition stars, I adapted the my
proof and calculations to Marchal's partition, and this resulted in a
much shorter final text.  The formalization project owes a great deal
to Marchal's improved geometric partition of space and to several of
his key lemmas.
% https://hal.archives-ouvertes.fr/file/index/docid/160350/filename/070615_Kepler.pdf




\section{Why HOL Light?}

I made the decision as a non-expert.  HOL Light had decisive
advantages: a well-developed theory of real analysis, no boundaries
between programming and proof script-writing, etc.

\section{tactics}

A HOL Light proof script is written directly as code in the
OCaml programming language, except that any phrase enclosed between
backquotes is parsed directly as a term in HOL Light.  


\section{Alignment between formal and informal}

(Euler example)

\section{A next generation proof}

The formalization of the Kepler conjecture was far from optimal.  This
final section points towards a second generation proof.

\subsection{nonlinear inequalities}
It was understood from the very beginning of the formalization of the
Kepler conjecture that the largest challenge would be the
formalization of the nonlinear inequalities.  Specifically,
floating-point arithmetic is slower by orders of magnitude when
executed as logical rules in HOL Light than in a native processor.

The proof of the Kepler conjecture relies on about a thousand
nonlinear inequalities over the real numbers.  These inequalities
are proved by computer using interval arithmetic. The inequalities
have the general form 
\begin{equation}\label{eqn:ineq}
\forall x \in D,\quad f_1(x) < 0 \lor f_2(x) < 0 \lor \cdots \lor f_k(x) < 0.
\end{equation}
Each function $f_i : R_i \to \ring{R}$ is defined on a subset $R_i$ of the
domain $D\subset \ring{R}^n$.
The inequality (\ref{eqn:ineq}) means more precisely that 
at each point $x\in D$, there exists $1\le i\le k$ such that
$x \in R_i$ and $f_i(x) < 0$. 

The domain $D$ is a product of 
compact intervals $[a,b]$, and $n$ is small (usually $n\le 6$).  
The dimension $n=6$ of the domain $D$ occurs naturally, because a
simplex in three dimensions is determined by its six edges.

There was an extraordinary amount of freedom in the creation
of a finite set of nonlinear inequalities that collectively imply
the Kepler conjecture.  In no sense is the current collection
optimal.  Further research is justified.

Starting in January 1994, I devoted all my efforts to the proof of the
Kepler conjecture.  My decision to put full energies behind the
conjecture was shaped by a key factor among others: I first learned in
1993 that interval arithmetic provided a reliable way to prove
nonlinear inequalities.  Sam Ferguson joined me, and much of our
research between 1995-1998 went into the development of efficient
algorithms for proving nonlinear inequalities.  Eventually, we settled
on using automatic differentiation to compute second order Taylor
expansions of the nonlinear functions with certified bounds on the
error terms.  When a Taylor expansion fails to give sufficient
accuracy on a given box-shaped domain, we branch and bound (by adaptively bisecting the
box and recursing on the smaller domains).  When a partial
derivative has fixed sign over the box, the function is
monotone, and the inequality is implied by a restricted inequality over a face
of the box, and the dimension drops.  (We do not claim
originality in these techniques, but considerable work went into
efficient implementation.)

Sam and I gave independent implementations of the algorithms (in C and
C++) that we used to cross-check our code for bugs. Sam finished his
thesis in 1997, but returned to University of Michigan for an
extended visit in 1998 to prove the final outstanding nonlinear
inequalities as we prepared to announce the solution.

To prepare for formalization, I ported the code to OCaml, and spent
months reworking the nonlinear inequalities until the programs
terminated in about 10 hours (of intensive floating-point
calculations).  In the end, the collection of nonlinear inequalites
used for the formalization is completely different from the collection
of inequalites that was used in the original 1998 proof.
The algorithms remained very close to those used earlier.

Solovyev was entirely responsible for the implementation of the
formalization in HOL Light.  As we understood all along, this was the
largest single challenge of the whole project.  An enormous amount of
code optimization was necessary to bring the formal calculation within
the reach of a large cloud computation.  Some of Solovyev's
optimizations included performing arithmetic in a large base rather
than base $2$, building massive addition and multiplication tables
inside HOL-Light so that each arithmetic operation could be done by a
simple look-up, precomputing the number of digits precision needed for
each calculation and using the minimum possible, computing all the
decision points informally and then replaying the decision-point
scripts in the formal verification, and caching intermediate
expressions to avoid recomputation.  He carefully crafted certain
inner loops to use the minimal number of primitive HOL Light
operations.

Solovyev's formalization has excellent benchmarks
when compared with formalizations of nonlinear inequalities in
other systems.  He has developed and document an independent tool
that can be used separately from the Flyspeck project.

Further major optimizations are certainly possible, and it would be an
interesting future project, to optimize to the point of moving the
entire formalization of the Kepler conjecture from the cloud to a
single laptop computer.  Some suggestions for further optimization
include implementing backchaining algorithms to compute partial
derivatives, exploiting common structures shared among inequalities to
verify inequalities in batches, precomputing second derivative bounds
on functions that appear in multiple inequalities (this was an
important technique in the 1998 proof that we later abandoned for no
good reason; in current the implementation, second derivative bounds
are wastefully recomputed every time the algorithm branches into two
sub-boxes), redesigning the collection of nonlinear inequalities,
precomputing lookup tables for inverse trig and other transcendental
functions, automatic generation of linear combinations (if $\sum_i a_i
f_i(x) < 0$ with $a_i\ge 0$, then for some $i$, we have the desired
conclusion $f_i(x) < 0$ in Equation (\ref{eqn:ineq})), better
decisions about partitioning boxes into sub-boxes, and generally any
number of techniques for global optimization developed during the past
twenty years (since our proof still relies on 1990s era technology).


The lion's share of the time is spent on computations near the
inequality \emph{``hot spots''}: small boxes where the functions
$f_i(x)$ are nearly $0$.  Speeding up the hot-spot computations would
bring significant gains.  One technique that should help would be to
center the Taylor expansion at the spot where $f_i(x)$ is maximized,
rather than at the center of the box (as we do now).

Machine learning might be tried.  We can view the space of proofs of
the Kepler conjecture as parametrized by various finite collections
$C$ of nonlinear inequalities.  To each collection $C$ there is a cost
as measured by the time required to verify the collection of
inequalities.  We wish to learn a collection $C$ that minimizes cost.

\subsection{Tame graphs}

The first major success in the formalization of the Kepler conjecture
was the Bauer-Nipkow formalization of the classification of tame
planar graphs in Isabelle.  (The word \emph{tame} has a technical
meaning in the proof.  A tame graph is one that satisfies a long list
of technical conditions that means that the graph encodes the
combinatorial structure of a potential counterexample to the Kepler
conjecture. Thus, the classification of tame planar graphs gives the
classification of possible counterexamples.)

The Bauer-Nipkow formalization was crucial in launching the Flyspeck
project and in galvinizing interest.  However, as the earliest part of
the project, it is currently the least compatible piece of the entire
Flyspeck project.  A reimplementation of their work is sorely needed.

The Bauer-Nipkow classification is the only part of the proof done in
Isabelle.  There have been many discussions about merging all parts of
the Flyspeck project into the same proof assistant, but this is still
work in progress.  Currently the statement of the classification
theorem is translated by hand from Isabelle to HOL Light, and the
translated statement is accepted as an unproved postulate in the HOL
Light part of the formalization.  The Isabelle proof is not translated
into HOL Light.  This hand translation is the weakest link in the
entire Flyspeck project.

As a step towards verifying the classification in HOL Light, in
unpublished work from 2018, Freek Wiedijk has produced an automated
translation of the tame graph classification from Isabelle to OCaml,
which John Harrison then (semi-manually) translated to definitions in HOL Light.
(Someone needs to check the compatibility of Harrison's translation with mine.)
Solovyev then created a tool that translates HOL Light equational theorems
into optimized executable OCaml code that computes using primitive inference rules.%
\footnote{\url{https://github.com/monadius/compute-hol-light}} His
experiments suggest that a verification of the tame graph
classification might be done in HOL Light in a 3000 hours computation.
(Isabelle is much faster because it exports code to ML, then executes
without the overhead of HOL primitive inferences.)
% Private communication 2018, R--Flyspeck.

A major inefficiency in the tame graph classification comes through
the definition of \emph{planar graph}.  The definition of planar graph
is treated as a black box in the Isabelle formalization: a planar
graph is any graph generated by algorithm $X$, where algorithm $X$ is
an edge-adding algorithm that I designed and implemented in
Mathematica (and later in Java) to generate all planar graphs (with
constraints on the number of faces, nodes, and degrees of the nodes)
around 1993.  The Bauer-Nipkow procedure filters the list of planar
graphs (generated by a black box algorithm) to produce the sublist of
tame graphs.  One of the cardinal rules of code verification is never
to verify code that has not been written with verification in mind.
The cardinal rule is violated for algorithm $X$.  It was a very
unpleasant experience for me to verify correctness in HOL Light of a
HOL Light translation of an Isabelle translation of an ML translation
of an old Java program written without formalization in mind.  In
retrospect, there would have been much better ways to proceed.  In
particular, the data structures used to represent planar graphs in
algorithm $X$ are entirely inappropriate for formalization.  (The data
encoding a graph is highly redundant, and the formalization has to
give a series of lemmas proving that the redundant data remains
\emph{in sync}.)  Moreover, other planar graph generating algorithms
are available, such as \emph{plaintri}.

















Tricks used.

Tricks not used.



Isabelle-HOL-Light interface.

Multiple list libraries and uniform style.

DRY Alignment.

Search - Zumkeller.

Tools - Libraries (Harrison), standalone nonlinear (Solovyev), linear programming (Solovyev).

Tactic styles (no distinction between Ocaml code and proof scripts).

Overlap of definitions (list libraries Isabelle, HOL Light, Coq).



\section{Concluding Remarks}

Some have expressed an ambition to use artificial intelligence to
\emph{solve math} in 
the coming years.  To the ambitious, we offer the more modest
challenge of using AI to develop a next-generation proof of the Kepler
conjecture.  The current proof is now over 22 years old.


\newpage

%\printbibliography

\bibliography{refs} 
\bibliographystyle{alpha}

\end{document}

