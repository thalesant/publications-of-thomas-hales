% Thomas Hales
% The Formal Proof of the Kepler Conjecture in Retrospective
% February 17, 2020. started.


%\documentclass[runningheads]{llncs}
%\documentclass{llncs}
\documentclass{amsart}

\usepackage{graphicx}
\usepackage{pdfpages}
%\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}
\usepackage{ellipsis}

% For code snippets
\usepackage{isabelle}
\usepackage{isabellesym}
\usepackage{isabelletags}
\usepackage{comment}
\usepackage{pdfsetup}
\usepackage{railsetup}
\usepackage{wasysym}

% Bibliography
% Change style to the requested style of the template
%\usepackage[backend=bibtex, style=numeric]{biblatex}
%\addbibresource{refs.bib}

% 
%tikz graphics
%\usepackage{xcolor} % to remove color.
\usepackage{tikz} % 
\usetikzlibrary{chains,shapes,arrows,%
 trees,matrix,positioning,decorations}
%\usepackage[framemethod=TikZ]{mdframed}

\def\tikzfig#1#2#3{%
\begin{figure}[htb]%
  \centering
\begin{tikzpicture}#3
\end{tikzpicture}
  \caption{#2}
  \label{fig:#1}%
\end{figure}%
}
\def\smalldot#1{\draw[fill=black] (#1) %
 node [inner sep=1.3pt,shape=circle,fill=black] {}}

%\newtheorem{theorem}{Theorem}[subsection]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{definition}[theorem]{Definition}

\newcommand{\ring}[1]{\mathbb{#1}}
\newcommand{\op}[1]{\hbox{#1}}

\title{The Formal Proof of the Kepler Conjecture in Retrospective}
\author{Thomas Hales}
\date{}
%\institute{University of Pittsburgh}
\date{February 20, 2020}

\begin{document}

\maketitle

\begin{abstract} 
  The Kepler conjecture asserts that no packing of congruent balls in
  Euclidean three-space has density greater than that of the
  face-centered cubic packing.  In 1998, Samuel Ferguson and Thomas
  Hales announced a computer-assisted proof of this conjecture.  Long
  delays in the refereeing process sparked a project to give a formal
  proof of the Kepler conjecture.  This article gives an overview of
  the formal proof of the Kepler Conjecture.
\end{abstract}

\parskip=0.8\baselineskip
\baselineskip=1.05\baselineskip

\newenvironment{blockquote}{%
  \par%
  \medskip%
  \baselineskip=0.7\baselineskip%
  \leftskip=2em\rightskip=2em%
  \noindent\ignorespaces}{%
  \par\medskip}

\section{Introduction}

In the late 16th century, Walter Raleigh asked his assistant Thomas
Harriot to compute the number of cannonballs piled in a pyramid (in
defense against the Spanish Armada).  Thomas Harriot obtained a
general formula (as the binomial coefficient $\choose{a}{b}$) for the
number of congruent balls piled in a pyramid of any size $k$ in any
dimension $d$.  In two dimensions, Harriot's formula reduces to the
formula $k(k+1)/2$ for triangular numbers.  In three dimensions,
Harriot's formula reduces to a formula known from antiquity in a
Sanskrit source.  The cannonball pyramid is known in chemistry as the
face-centered cubic packing.

Harriot was an early proponent of the atomic theory, and he viewed
atoms as small balls -- which might almost be viewed as miniature
cannonballs.  Harriot and Kepler shared an interest in optics, and
they corresponded during the first decade of the 17th century.
Harriot pushed the atomic theory in their correspondence, but Kepler
remained skeptical.

However, one evening crossing the Charles bridge in Prague, as it
started to snow, Kepler started to contemplate why snowflakes have six
sides.  His contemplations led to a booklet ``The Six-Cornered
Snowflake,'' which can be described as an early essay in
crystallography, explaining observable symmetries in nature by
arrangements of invisibly small particles.  In this booklet, published
in 1611, Kepler describes the face-centered cubic packing and asserts
that it will be the ``tightest possible, so that in no other
arrangement can more pellets be stuffed into the same space.'' The
notion of density was suggested to him by the tightly packed seeds of
a pomegranite, and the face-centered cubic packing was suggested to
him by the structure of honeycomb cells.

Kepler's assertion has come to be known as the Kepler conjecture.
Hilbert made the Kepler conjecture part of the 18th problem, one among
the influential list of problems he proposed at the international
congress in Paris in 1900.

The two-dimensional analogue of the Kepler conjecture asserts that the
densest packing of congruent circular disks in the plane is achieved
by the hexagonal lattice packing.  A. Thue is often credited with the
proof, although the first careful proofs do not appear until much
later.

The first detailed strategy to prove the Kepler conjecture was
formulated by L. Fejes T\'oth in 1953.  He was also the first to
suggest a computer-assisted proof.

Over the years, there were some notorious false claims of a proof.
Buckminster Fuller (the creator of the geodesic dome) claimed a proof,
but his claim lacked any substance.  Around 1990, a Berkeley professor
W.-Y. Hsiang claimed a proof.  In rebuttal to his claimed proof, in a
heated debate, I published a paper containing explicit counterexamples
to his work.  For a few years in the early 1990s, the research area
became toxic, until the debate was eventually settled in my favor.

Samuel Ferguson and I annouced a solution to the Kepler conjecture in
August 1998.  Our solution was contained in a series of preprints
posted to the ArXiv.  Because of the conjecture's notorious history, I
had hoped that publication of our papers would be swift. This was not
to be.  A panel of 12 referees was assigned the review.  Because of
lingering doubts among the referees, the full publication of the proof
did not occur until 2006, nearly eight years after submission.

During those years in delay, growing out of my frustration, in an
effort to bypass the referees, I launched a project to give a formal
proof of the Kepler conjecture.  Although the project was born out of
frustration, in truth, it was also
shaped by a broader vision of the central importance of
computer-assisted mathematics in years to come.  In a large group
collaboration, the formal proof of the Kepler conjecture was completed
in 2014.

\section{Why HOL Light?}

I made the decision as a non-expert.  HOL Light had decisive
advantages: a well-developed theory of real analysis, no boundaries
between programming and proof script-writing, etc.

\section{Alignment between formal and informal}

(Euler example)

\section{A next generation proof}

The formalization of the Kepler conjecture was far from optimal.  This
final section points towards a second generation proof.

\subsection{nonlinear inequalities}
It was understood from the very beginning of the formalization of the
Kepler conjecture that the largest challenge would be the
formalization of the nonlinear inequalities.  Specifically,
floating-point arithmetic is slower by orders of magnitude when
executed as logical rules in HOL Light than in a native processor.

The proof of the Kepler conjecture relies on about a thousand
nonlinear inequalities over the real numbers.  These inequalities
are proved by computer using interval arithmetic. The inequalities
have the general form 
\begin{equation}\label{eqn:ineq}
\forall x \in D,\quad f_1(x) < 0 \lor f_2(x) < 0 \lor \cdots \lor f_k(x) < 0.
\end{equation}
Each function $f_i : R_i \to \ring{R}$ is defined on a subset $R_i$ of the
domain $D\subset \ring{R}^n$.
The inequality (\ref{eqn:ineq}) means more precisely that 
at each point $x\in D$, there exists $1\le i\le k$ such that
$x \in R_i$ and $f_i(x) < 0$. 

The domain $D$ is a product of 
compact intervals $[a,b]$, and $n$ is small (usually $n\le 6$).  
The dimension $n=6$ of the domain $D$ occurs naturally, because a
simplex in three dimensions is determined by its six edges.

There was an extraordinary amount of freedom in the creation
of a finite set of nonlinear inequalities that collectively imply
the Kepler conjecture.  In no sense is the current collection
optimal.  Further research is justified.

Starting in January 1994, I devoted all my efforts to the proof of the
Kepler conjecture.  My decision to put full energies behind the
conjecture was shaped by a key factor among others: I first learned in
1993 that interval arithmetic provided a reliable way to prove
nonlinear inequalities.  Sam Ferguson joined me, and much of our
research between 1995-1998 went into the development of efficient
algorithms for proving nonlinear inequalities.  Eventually, we settled
on using automatic differentiation to compute second order Taylor
expansions of the nonlinear functions with certified bounds on the
error terms.  When a Taylor expansion fails to give sufficient
accuracy on a given box-shaped domain, we branch and bound (by adaptively bisecting the
box and recursing on the smaller domains).  When a partial
derivative has fixed sign over the box, the function is
monotone, and the inequality is implied by a restricted inequality over a face
of the box, and the dimension drops.  (We do not claim
originality in these techniques, but considerable work went into
efficient implementation.)

Sam and I gave independent implementations of the algorithms (in C and
C++) that we used to cross-check our code for bugs. Sam finished his
thesis in 1997, but returned to University of Michigan for an
extended visit in 1998 to prove the final outstanding nonlinear
inequalities as we prepared to announce the solution.

To prepare for formalization, I ported the code to OCaml, and spent
months reworking the nonlinear inequalities until the programs
terminated in about 10 hours (of intensive floating-point
calculations).  In the end, the collection of nonlinear inequalites
used for the formalization is completely different from the collection
of inequalites that was used in the original 1998 proof.
The algorithms remained very close to those used earlier.

Solovyev was entirely responsible for the implementation of the
formalization in HOL Light.  As we understood all along, this was the
largest single challenge of the whole project.  An enormous amount of
code optimization was necessary to bring the formal calculation within
the reach of a large cloud computation.  Some of Solovyev's
optimizations included performing arithmetic in a large base rather
than base $2$, building massive addition and multiplication tables
inside HOL-Light so that each arithmetic operation could be done by a
simple look-up, precomputing the number of digits precision needed for
each calculation and using the minimum possible, computing all the
decision points informally and then replaying the decision-point
scripts in the formal verification, and caching intermediate
expressions to avoid recomputation.  He carefully crafted certain
inner loops to use the minimal number of primitive HOL Light
operations.

Solovyev's formalization has excellent benchmarks
when compared with formalizations of nonlinear inequalities in
other systems.  He has developed and document an independent tool
that can be used separately from the Flyspeck project.

Further major optimizations are certainly possible, and it would be an
interesting future project, to optimize to the point of moving the
entire formalization of the Kepler conjecture from the cloud to a
single laptop computer.  Some suggestions for further optimization
include implementing backchaining algorithms to compute partial
derivatives, exploiting common structures shared among inequalities to
verify inequalities in batches, precomputing second derivative bounds
on functions that appear in multiple inequalities (this was an
important technique in the 1998 proof that we later abandoned for no
good reason; in current the implementation, second derivative bounds
are wastefully recomputed every time the algorithm branches into two
sub-boxes), redesigning the collection of nonlinear inequalities,
precomputing lookup tables for inverse trig and other transcendental
functions, automatic generation of linear combinations (if $\sum_i a_i
f_i(x) < 0$ with $a_i\ge 0$, then for some $i$, we have the desired
conclusion $f_i(x) < 0$ in Equation (\ref{eqn:ineq})), better
decisions about partitioning boxes into sub-boxes, and generally any
number of techniques for global optimization developed during the past
twenty years (since our proof still relies on 1990s era technology).


The lion's share of the time is spent on computations near the
inequality \emph{``hot spots''}: small boxes where the functions
$f_i(x)$ are nearly $0$.  Speeding up the hot-spot computations would
bring significant gains.  One technique that should help would be to
center the Taylor expansion at the spot where $f_i(x)$ is maximized,
rather than at the center of the box (as we do now).

Machine learning might be tried.  We can view the space of proofs of
the Kepler conjecture as parametrized by various finite collections
$C$ of nonlinear inequalities.  To each collection $C$ there is a cost
as measured by the time required to verify the collection of
inequalities.  We wish to learn a collection $C$ that minimizes cost.

\subsection{Tame graphs}

The first major success in the formalization of the Kepler conjecture
was the Bauer-Nipkow formalization of the classification of tame
planar graphs in Isabelle.  (The word \emph{tame} has a technical
meaning in the proof.  A tame graph is one that satisfies a long list
of technical conditions that means that the graph encodes the
combinatorial structure of a potential counterexample to the Kepler
conjecture. Thus, the classification of tame planar graphs gives the
classification of possible counterexamples.)

The Bauer-Nipkow formalization was crucial in launching the Flyspeck
project and in galvinizing interest.  However, as the earliest part of
the project, it is currently the least compatible piece of the entire
Flyspeck project.  A reimplementation of their work is sorely needed.

The Bauer-Nipkow classification is the only part of the proof done in
Isabelle.  There have been many discussions about merging all parts of
the Flyspeck project into the same proof assistant, but this is still
work in progress.  Currently the statement of the classification
theorem is translated by hand from Isabelle to HOL Light, and the
translated statement is accepted as an unproved postulate in the HOL
Light part of the formalization.  The Isabelle proof is not translated
into HOL Light.  This hand translation is the weakest link in the
entire Flyspeck project.

As a step towards verifying the classification in HOL Light, in
unpublished work from 2018, Freek Wiedijk has produced an automated
translation of the tame graph classification from Isabelle to OCaml,
which John Harrison then (semi-manually) translated to definitions in HOL Light.
(Someone needs to check the compatibility of Harrison's translation with mine.)
Solovyev then created a tool that translates HOL Light equational theorems
into optimized executable OCaml code that computes using primitive inference rules.%
\footnote{\url{https://github.com/monadius/compute-hol-light}} His
experiments suggest that a verification of the tame graph
classification might be done in HOL Light in a 3000 hours computation.
(Isabelle is much faster because it exports code to ML, then executes
without the overhead of HOL primitive inferences.)
% Private communication 2018, R--Flyspeck.

A major inefficiency in the tame graph classification comes through
the definition of \emph{planar graph}.  The definition of planar graph
is treated as a black box in the Isabelle formalization: a planar
graph is any graph generated by algorithm $X$, where algorithm $X$ is
an edge-adding algorithm that I designed and implemented in
Mathematica (and later in Java) to generate all planar graphs (with
constraints on the number of faces, nodes, and degrees of the nodes)
around 1993.  The Bauer-Nipkow procedure filters the list of planar
graphs (generated by a black box algorithm) to produce the sublist of
tame graphs.  One of the cardinal rules of code verification is never
to verify code that has not been written with verification in mind.
The cardinal rule is violated for algorithm $X$.  It was a very
unpleasant experience for me to verify correctness in HOL Light of a
HOL Light translation of an Isabelle translation of an ML translation
of an old Java program written without formalization in mind.  In
retrospect, there would have been much better ways to proceed.  In
particular, the data structures used to represent planar graphs in
algorithm $X$ are entirely inappropriate for formalization.  (The data
encoding a graph is highly redundant, and the formalization has to
give a series of lemmas proving that the redundant data remains
\emph{in sync}.)  Moreover, other planar graph generating algorithms
are available, such as \emph{plaintri}.

















Tricks used.

Tricks not used.



Isabelle-HOL-Light interface.

Multiple list libraries and uniform style.

DRY Alignment.

Search - Zumkeller.

Tools - Libraries (Harrison), standalone nonlinear (Solovyev), linear programming (Solovyev).

Tactic styles (no distinction between Ocaml code and proof scripts).

Overlap of definitions (list libraries Isabelle, HOL Light, Coq).



\section{Concluding Remarks}

Some have expressed an ambition to use artificial intelligence to
\emph{solve math} in 
the coming years.  To the ambitious, we offer the more modest
challenge of using AI to develop a next-generation proof of the Kepler
conjecture.  The current proof is now over 22 years old.


\newpage

%\printbibliography

\bibliography{refs} 
\bibliographystyle{alpha}

\end{document}

